{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install timm","metadata":{"execution":{"iopub.status.busy":"2022-04-01T16:32:21.158401Z","iopub.execute_input":"2022-04-01T16:32:21.158704Z","iopub.status.idle":"2022-04-01T16:32:30.431579Z","shell.execute_reply.started":"2022-04-01T16:32:21.158672Z","shell.execute_reply":"2022-04-01T16:32:30.430394Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# coding: utf-8\nimport logging\nimport os\nfrom PIL import Image\nimport cv2\nimport numpy as np\nimport timm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom sklearn import metrics\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset\nfrom torch.utils.data.dataloader import DataLoader\nfrom torchvision import transforms\n\nlabels = {\n    'Forest':0, \n    'bedroom':1, \n    'Office':2, \n    'Highway':3, \n    'Coast':4, \n    'Insidecity':5, \n    'TallBuilding':6,\n    'industrial':7,\n    'Street':8, \n    'livingroom':9,\n    'Suburb':10, \n    'Mountain':11, \n    'kitchen':12, \n    'OpenCountry':13, \n    'store':14\n    }\n\nCONFIG = {\n    \"seed\": 3047,\n    \"epochs\":20,\n    \"img_size\": 256,\n    #           \"model_name\": \"efficientnet_b3a\",\n    \"model_1\":\"efficientnet_b3a\",\n    \"model_2\":\"dla60_res2net\",\n    \"model_3\":\"mobilenetv3_small_050\",\n    \"model_4\":\"gluon_resnet34_v1b\",\n    \"num_classes\": 15,\n    \"train_batch_size\": 16,\n    \"valid_batch_size\": 16,\n    \"learning_rate\": 1e-3,\n    'T_0': 5,\n    \"eta_min\": 1e-4,\n    \"T_max\": 500,\n    'T_mult':2,\n    \"weight_decay\": 1e-6,\n    \"n_fold\": 5,\n    \"n_accumulate\": 1,\n    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n    \"test_mode\":True, # enable for testing pipeline, changes epochs to 2 and uses just 100 training samples\n    \"enable_amp_half_precision\": False, # Try it in your local machine (the code is made for working with !pip install apex, not the pytorch native apex)\n    # ArcFace Hyperparameters\n    \"s\": 30.0, \n    \"m\": 0.30,\n    \"ls_eps\": 0.0,\n    \"easy_margin\": False\n}\n\ndef set_seed(seed=42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-01T16:32:30.435394Z","iopub.execute_input":"2022-04-01T16:32:30.435891Z","iopub.status.idle":"2022-04-01T16:32:30.451452Z","shell.execute_reply.started":"2022-04-01T16:32:30.435842Z","shell.execute_reply":"2022-04-01T16:32:30.450316Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class SceneDatasets(Dataset):\n    def __init__(self, txt_path, transform = None, target_transform = None):\n        fh = open(txt_path, 'r')\n        imgs = []\n        for line in fh:\n            line = line.rstrip()\n            words = line.split()\n            imgs.append((words[0], int(words[1])))\n            self.imgs = imgs \n            self.transform = transform\n            self.target_transform = target_transform\n    def __getitem__(self, index):\n        imgPath, label = self.imgs[index]\n        img = Image.open(imgPath).convert(\"RGB\") # 灰度图转RGB, 以适应预训练的模型\n        if self.transform is not None:\n            img = self.transform(img) \n        return img, label\n    def __len__(self):\n        return len(self.imgs)    \n\ntrainingDatasetPath = '../input/datapath/training_kaggle.txt'\ntestDatasetPath = '../input/datapath/testing_kaggle.txt'\n\ntraining_transform = transforms.Compose([\n    # transforms.RandomCrop(32, padding=4),\n    transforms.Resize((CONFIG['img_size'],CONFIG['img_size'])),\n#     transforms.ColorJitter(brightness=0.5),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.ToTensor(),\n    transforms.Normalize(0.45526364,0.24906044),\n])\n\nval_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(0.45526364,0.24906044),\n])\n\nsceneDatasets = SceneDatasets(trainingDatasetPath,training_transform)\n\ntrain_size = int(len(sceneDatasets) * 0.8) # 8:2 数据集分割\ntest_size = len(sceneDatasets) - train_size\ntrain_dataset, val_dataset = torch.utils.data.random_split(sceneDatasets, [train_size, test_size])\n\ntrain_loader = DataLoader(train_dataset,batch_size=CONFIG['train_batch_size'], num_workers=2)\nval_loader = DataLoader(val_dataset,batch_size=CONFIG['valid_batch_size'], num_workers=2)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-01T16:32:30.453633Z","iopub.execute_input":"2022-04-01T16:32:30.453973Z","iopub.status.idle":"2022-04-01T16:32:30.479740Z","shell.execute_reply.started":"2022-04-01T16:32:30.453930Z","shell.execute_reply":"2022-04-01T16:32:30.478586Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nfrom matplotlib import pyplot as plt\nfor img, label in train_loader:\n    plt.imshow(img[0][0])\n    print(list(labels.keys())[list(labels.values()).index(label[0])])\n    break\n","metadata":{"execution":{"iopub.status.busy":"2022-04-01T16:32:30.484267Z","iopub.execute_input":"2022-04-01T16:32:30.484788Z","iopub.status.idle":"2022-04-01T16:32:31.198915Z","shell.execute_reply.started":"2022-04-01T16:32:30.484755Z","shell.execute_reply":"2022-04-01T16:32:31.197945Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class GeM(nn.Module):\n    def __init__(self, p=3, eps=1e-6):\n        super(GeM, self).__init__()\n        self.p = nn.Parameter(torch.ones(1)*p)\n        self.eps = eps\n\n    def forward(self, x):\n        return self.gem(x, p=self.p, eps=self.eps)\n        \n    def gem(self, x, p=3, eps=1e-6):\n        return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n        \n    def __repr__(self):\n        return self.__class__.__name__ + \\\n                '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n                ', ' + 'eps=' + str(self.eps) + ')'\n\nclass ArcMarginProduct(nn.Module):\n    r\"\"\"Implement of large margin arc distance: :\n        Args:\n            in_features: size of each input sample\n            out_features: size of each output sample\n            s: norm of input feature\n            m: margin\n            cos(theta + m)\n        \"\"\"\n    def __init__(self, in_features, out_features, s=30.0, \n                 m=0.50, easy_margin=False, ls_eps=0.0):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.s = s\n        self.m = m\n        self.ls_eps = ls_eps  # label smoothing\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = torch.cos(m)\n        self.sin_m = torch.sin(m)\n        self.th = torch.cos(torch.pi - m)\n        self.mm = torch.sin(torch.pi - m) * m\n\n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        if(CONFIG['enable_amp_half_precision']==True):\n            cosine = cosine.to(torch.float32)\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------\n        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n        one_hot = torch.zeros(cosine.size(), device=CONFIG['device'])\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) ------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n\n        return output\n\nclass SceneModel(nn.Module):\n    def __init__(self, model_name, pretrained=True):\n        super(SceneModel, self).__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained,)\n        in_features = self.model.classifier.in_features # model_1\n#         in_features = self.model.fc.in_features # model_4\n        self.model.reset_classifier(0)\n#         self.model.classifier = nn.Identity()\n        self.model.global_pool = nn.Identity()\n        self.pooling = GeM()\n        self.drop = nn.Dropout(p=0.2, inplace=False)\n        self.fc = nn.Linear(in_features, CONFIG[\"num_classes\"])\n\n#         self.arc = ArcMarginProduct(512, \n#                            CONFIG[\"num_classes\"],\n#                            s=CONFIG[\"s\"], \n#                            m=CONFIG[\"m\"], \n#                            easy_margin=CONFIG[\"ls_eps\"], \n#                            ls_eps=CONFIG[\"ls_eps\"])\n        \n    def forward(self, images):\n        features = self.model(images)\n        pooled_features = self.pooling(features).flatten(1)\n        pooled_drop = self.drop(pooled_features)\n        emb = self.fc(pooled_drop)\n#         output = self.arc(emb,labels)\n        return emb\n","metadata":{"execution":{"iopub.status.busy":"2022-04-01T16:36:34.612274Z","iopub.execute_input":"2022-04-01T16:36:34.612633Z","iopub.status.idle":"2022-04-01T16:36:34.637850Z","shell.execute_reply.started":"2022-04-01T16:36:34.612602Z","shell.execute_reply":"2022-04-01T16:36:34.636516Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"from tqdm import trange\nfrom tqdm import tqdm\nmodel = SceneModel(CONFIG['model_1'])\n# print(model)\nmodel.to(CONFIG['device'])\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'], )\n# optimizer = optim.SGD(model.parameters(), lr=CONFIG['learning_rate'], \n#                        momentum=0.9)\nscheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=CONFIG['T_0'], T_mult=CONFIG['T_mult'], eta_min=CONFIG['eta_min'])\n\nlogger = logging.getLogger(\"template_model.train\")\nlogger.info(\"Start training\")\n\nfor epoch in trange(CONFIG['epochs'], desc='epochs'):\n    # train\n    for batch in tqdm(train_loader, desc='batch'):\n        images = batch[0].to(CONFIG['device'], )\n        labels = batch[1].to(CONFIG['device'], )\n        optimizer.zero_grad()\n        optimizer.step()\n        predicts = model(images)\n        loss = loss_fn(predicts,labels)\n        logger.info(\"Epoch[{}] Iteration[{}/{}] Loss: {:.2f}\"\n                        .format(epoch, iter, len(train_loader), loss.item()))\n        loss.backward()\n        scheduler.step()\n\n    if (epoch % 10)==0:\n        with torch.no_grad():\n            for batch in val_loader:\n                images = batch[0].to(CONFIG['device'], )\n                labels = batch[1].to(CONFIG['device'], )\n                predicts = model(images)\n                accuracy_score = metrics.accuracy_score(labels.cpu(), F.softmax(predicts).argmax(1).cpu())\n                print(accuracy_score)\n# #                 report = metrics.classification_report(batch[1], predicts, target_names=labels)\n#                 logger.info(\"Epoch[{}] Iteration[{}/{}] Acc: {:.2f}\"\n#                         .format(epoch, iter, len(train_loader), accuracy_score))\n","metadata":{"execution":{"iopub.status.busy":"2022-04-01T16:45:21.405930Z","iopub.execute_input":"2022-04-01T16:45:21.406634Z","iopub.status.idle":"2022-04-01T16:46:02.034867Z","shell.execute_reply.started":"2022-04-01T16:45:21.406591Z","shell.execute_reply":"2022-04-01T16:46:02.033243Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}